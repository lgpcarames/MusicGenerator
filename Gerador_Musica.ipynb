{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b82241",
   "metadata": {},
   "source": [
    "# Gerador de Música"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f1f2b7",
   "metadata": {},
   "source": [
    "Este notebook foi criado com o objetivo de criar um gerador de música utilizando os passos apresentados [neste artigo](https://www.analyticsvidhya.com/blog/2020/01/how-to-perform-automatic-music-generation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed6361",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06079939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 16:14:55.132426: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-18 16:14:55.133187: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import random\n",
    "from music21 import *\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from keras.layers import BatchNormalization as BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7b560",
   "metadata": {},
   "source": [
    "Infelizmente os arquivos do artigo mencionado não estão mais disponíveis, mas eu encontrei um site que possui uma base de dados com um escopo similar, que pode nos ajudar a obter resultados bons também. Como são muitos arquivos para fazer o download, então eu irei utilizar o beautifulsoup para poder realizar o download de todos os arquivos de maneira rápida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b23f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acessar o html da pagina pelo bs4\n",
    "# obter o url dos arquivos mid a partir do html\n",
    "html = urlopen(\"https://www.audiolabs-erlangen.de/resources/MIR/SMD/midi\")\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "urls = []\n",
    "names = []\n",
    "for i, link in enumerate(bs.findAll(\"a\")):\n",
    "    if link.get(\"href\").endswith(\"mid\"):\n",
    "        urls.append(link.get(\"href\"))\n",
    "\n",
    "\n",
    "for i in range(len(urls)):\n",
    "    names.append(urls[i].split('/')[-1:])\n",
    "# realizar o download dos arquivos mid utilizando as url obtidas no passo anterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c0d49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(urls)):\n",
    "    r = requests.get(urls[i])\n",
    "    name = urls[i].split('/')[-1:][0]\n",
    "    \n",
    "    open('./musicas/'+ name, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68610d85",
   "metadata": {},
   "source": [
    "Com os arquivos devidamente baixados, agora definiremos uma função que irá ler os arquivos mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d7d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_midi(file):\n",
    "    print(\"Carregando arquivos de música: \", file)\n",
    "    \n",
    "    notes = []\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    # analisando um arquivo midi\n",
    "    \n",
    "    midi = converter.parse(file)\n",
    "    \n",
    "    # agrupando baseado em diferentes instrumentos\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "    \n",
    "    # Realizando o loop sobre todos os instrumentos\n",
    "    for part in s2.parts:\n",
    "        \n",
    "        # selecionando os elementos de um único piano\n",
    "#         if 'Piano' in str(part):\n",
    "        notes_to_parse = part.recurse()\n",
    "\n",
    "        # Encontrando se um particular elemento é uma nota ou um acorde\n",
    "        for element in notes_to_parse:\n",
    "            # notas\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "\n",
    "            if isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "                    \n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee9a79a",
   "metadata": {},
   "source": [
    "Agora, iremos carregar os arquivos MIDI no nosso ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "239025c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando arquivos de música:  ./musicas/Chopin_Op028-17_005_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bach_BWV875-02_002_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Brahms_Op010No1_003_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bach_BWV849-01_001_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op026No2_005_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op029_004_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Beethoven_Op031No2-03_002_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bartok_SZ080-01_002_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Liszt_VariationenBachmotivWeinenKlagenSorgenZagen_001_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bach_BWV888-01_008_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bartok_SZ080-02_002_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Rachmaninoff_Op036-02_007_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Beethoven_Op031No2-01_002_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op028-01_003_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op028-04_003_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Haydn_HobXVINo52-03_008_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op028-11_003_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bartok_SZ080-03_002_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Liszt_AnnesDePelerinage-LectureDante_002_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op028-15_006_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op026No1_003_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Haydn_HobXVINo52-02_008_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Beethoven_Op027No1-01_003_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bach_BWV875-01_002_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Beethoven_WoO080_001_20081107-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Rachmaninov_Op039No1_002_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Brahms_Op005-01_002_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Haydn_HobXVINo52-01_008_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bach_BWV871-02_002_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Ravel_ValsesNoblesEtSentimentales_003_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Haydn_Hob017No4_003_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bach_BWV871-01_002_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Beethoven_Op027No1-03_003_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Ravel_JeuxDEau_008_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Rachmaninoff_Op036-01_007_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Rachmaninoff_Op036-03_007_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op010-04_007_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op010-03_007_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Liszt_KonzertetuedeNo2LaLeggierezza_003_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Mozart_KV398_002_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op066_006_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op048No1_007_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Skryabin_Op008No8_003_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Brahms_Op010No2_003_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Chopin_Op028-03_003_20100611-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Beethoven_Op027No1-02_003_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Beethoven_Op031No2-02_002_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bach_BWV888-02_008_20110315-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Bach_BWV849-02_001_20090916-SMD.mid\n",
      "Carregando arquivos de música:  ./musicas/Mozart_KV265_006_20110315-SMD.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61414/1460871410.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  notes_array = np.array([read_midi(path+i) for i in files])\n"
     ]
    }
   ],
   "source": [
    "path = './musicas/'\n",
    "\n",
    "\n",
    "# lendo o nome de todos os arquivos\n",
    "files = [i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "# lendo cada arquivo mid\n",
    "notes_array = np.array([read_midi(path+i) for i in files])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23e059",
   "metadata": {},
   "source": [
    "### Entendendo os dados\n",
    "\n",
    "Nesta seção, iremos explorar o conjunto de dados e entendê-lo em mais detalhes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b199ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219\n"
     ]
    }
   ],
   "source": [
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "unique_notes = list(set(notes_))\n",
    "\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2326ed",
   "metadata": {},
   "source": [
    "Como você pode ver, o número de notas únicas é 1219. Agora vejamos a distribuição das notas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13e0de80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.084e+03, 4.300e+01, 3.900e+01, 2.300e+01, 8.000e+00, 9.000e+00,\n",
       "        8.000e+00, 3.000e+00, 1.000e+00, 1.000e+00]),\n",
       " array([1.0000e+00, 1.6260e+02, 3.2420e+02, 4.8580e+02, 6.4740e+02,\n",
       "        8.0900e+02, 9.7060e+02, 1.1322e+03, 1.2938e+03, 1.4554e+03,\n",
       "        1.6170e+03]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 326
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq = dict(Counter(notes_))\n",
    "\n",
    "no = [count for _, count in freq.items()]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d472d37",
   "metadata": {},
   "source": [
    "Do histograma acima, podemos notar que a maioria das notas ocorrem com pouquíssima frequência. Então, podemos apenas manter as notas com maior frequência e ignorar as demais. Vamos utilizar apenas as que possuem frequência igual ou acima de 50,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff410678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cadb560",
   "metadata": {},
   "source": [
    "Como podemos ver aqui, o número de notas que com frequência maior ou igual a 50 é 204. Agora vamos preparar novos arquivos de música que irão conter apenas as notas mais frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4fe49cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61414/1226839680.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  new_music = np.array(new_music)\n"
     ]
    }
   ],
   "source": [
    "new_music = []\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp = []\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)\n",
    "    new_music.append(temp)\n",
    "    \n",
    "    \n",
    "new_music = np.array(new_music)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fae939",
   "metadata": {},
   "source": [
    "### Preparando os dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f6cee",
   "metadata": {},
   "source": [
    "Para entender como os dados devem ser preparados, devemos entender como o modelo será estruturado.\n",
    "\n",
    "Imagine que temos um trecho de áudio de 5 segundos com uma taxa de 16 mil amostras por segundo, isso dará no total 80 mil amostras ao longo de todo o trecho do áudio. Se quebrarmos todo o áudio em pedaços iguais de 1024 amostras, podemos alimentar uma rede neural com estas amostras, de modo que a rede neural possa prever qual seria o valor da amostra n° 1025, por exemplo.\n",
    "\n",
    "Esse processo é chamado de **autoregressivo**, portanto, o modelo a utilizá-lo é conhecido como **modelo autoregressivo**.\n",
    "\n",
    "Tendo esses passos esclarecidos, podemos então definir como funcionará o processo de inferência que será utilizado para construir a música. Consideremos do exemplo anterior, que as amostras dívididas em \"pedaços\", são armazenadas, na realidade, em vetores. Com isso, os passos serão:\n",
    "1. Selecionar um vetor de amostras como ponto de partida do modelo;\n",
    "- Inicialmente, iremos criar a coleção de vetores de amostras, sorteando dentro desta coleção um vetor em específico para que o modelo faça a previsão.\n",
    "2. O modelo irá gerar como saída uma distribuição de probabilidade para cada amostra;\n",
    "3. A amostra com a maior probabilidade será escolhida e anexada em um novo vetor de amostras;\n",
    "4. Deleta o primeiro elemento e passa o valor previsto como entrada para a próxima iteração;\n",
    "5. Repete o processo dos passos 2 ao 4 por um dado número de iterações\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e87537ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_timesteps = 70\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        # preparando as sequências de entrada e saída:\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e2136",
   "metadata": {},
   "source": [
    "Agora, iremos atribuir um único inteiro para cada nota:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdc6d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3c1a3",
   "metadata": {},
   "source": [
    "Iremos preparara a sequência de dados de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67be800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq = []\n",
    "\n",
    "for i in x:\n",
    "    temp = []\n",
    "    \n",
    "    for j in i:\n",
    "        # atrinbuindo um inteiro único para cada nota\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3729957",
   "metadata": {},
   "source": [
    "De maneira similar, realizamos o mesmo procedimento agora para os dados de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d13db491",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_,number) for number, note_ in enumerate(unique_y))\n",
    "y_seq = np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48daf619",
   "metadata": {},
   "source": [
    "Iremos preservar apenas 80% dos dados de treinamento e os outros 20% serão utilizados para avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ab62c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq, y_seq, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2c6af",
   "metadata": {},
   "source": [
    "## Aplicação dos Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a201e",
   "metadata": {},
   "source": [
    "### Modelo WaveNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb14c1",
   "metadata": {},
   "source": [
    "Definindo a arquitetura WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "293392e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 100)           20400     \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 32, 64)            19264     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 16, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 16, 128)           24704     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 8, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 8, 256)            98560     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 256)            0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 4, 256)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 204)               52428     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 281,148\n",
      "Trainable params: 281,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 23:27:53.178579: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-16 23:27:53.178615: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-16 23:27:53.178639: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lucas-Inspiron-5566): /proc/driver/nvidia/version does not exist\n",
      "2022-03-16 23:27:53.178856: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32, trainable=True))\n",
    "\n",
    "model.add(Conv1D(64, 3, padding='causal', activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(128, 3, activation='relu', dilation_rate=2, padding='causal'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256, 3, activation='relu', dilation_rate=4, padding='causal'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(GlobalMaxPool1D())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341c634",
   "metadata": {},
   "source": [
    "Definindo o callback para salvar o melhor modelo durante o treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b3f1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9f290",
   "metadata": {},
   "source": [
    "Vamos treinar o modelo com o batch size de 64 para 100 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7a11c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 4.8088\n",
      "Epoch 1: val_loss improved from inf to 4.71754, saving model to best_model.h5\n",
      "913/913 [==============================] - 23s 24ms/step - loss: 4.8088 - val_loss: 4.7175\n",
      "Epoch 2/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 4.5410\n",
      "Epoch 2: val_loss improved from 4.71754 to 4.45842, saving model to best_model.h5\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 4.5409 - val_loss: 4.4584\n",
      "Epoch 3/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 4.3404\n",
      "Epoch 3: val_loss improved from 4.45842 to 4.33443, saving model to best_model.h5\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 4.3403 - val_loss: 4.3344\n",
      "Epoch 4/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 4.2298\n",
      "Epoch 4: val_loss improved from 4.33443 to 4.28926, saving model to best_model.h5\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 4.2301 - val_loss: 4.2893\n",
      "Epoch 5/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 4.1436\n",
      "Epoch 5: val_loss improved from 4.28926 to 4.23795, saving model to best_model.h5\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 4.1436 - val_loss: 4.2380\n",
      "Epoch 6/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 4.0715\n",
      "Epoch 6: val_loss improved from 4.23795 to 4.21098, saving model to best_model.h5\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 4.0715 - val_loss: 4.2110\n",
      "Epoch 7/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 4.0064\n",
      "Epoch 7: val_loss improved from 4.21098 to 4.17861, saving model to best_model.h5\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 4.0063 - val_loss: 4.1786\n",
      "Epoch 8/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.9418\n",
      "Epoch 8: val_loss improved from 4.17861 to 4.15584, saving model to best_model.h5\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.9418 - val_loss: 4.1558\n",
      "Epoch 9/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.8830\n",
      "Epoch 9: val_loss improved from 4.15584 to 4.15372, saving model to best_model.h5\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.8829 - val_loss: 4.1537\n",
      "Epoch 10/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.8321\n",
      "Epoch 10: val_loss improved from 4.15372 to 4.11763, saving model to best_model.h5\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.8320 - val_loss: 4.1176\n",
      "Epoch 11/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.7829\n",
      "Epoch 11: val_loss improved from 4.11763 to 4.10959, saving model to best_model.h5\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.7829 - val_loss: 4.1096\n",
      "Epoch 12/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.7348\n",
      "Epoch 12: val_loss did not improve from 4.10959\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.7344 - val_loss: 4.1186\n",
      "Epoch 13/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.6909\n",
      "Epoch 13: val_loss improved from 4.10959 to 4.10349, saving model to best_model.h5\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.6911 - val_loss: 4.1035\n",
      "Epoch 14/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.6441\n",
      "Epoch 14: val_loss did not improve from 4.10349\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.6440 - val_loss: 4.1309\n",
      "Epoch 15/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.6134\n",
      "Epoch 15: val_loss improved from 4.10349 to 4.10276, saving model to best_model.h5\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.6134 - val_loss: 4.1028\n",
      "Epoch 16/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.5769\n",
      "Epoch 16: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.5769 - val_loss: 4.1167\n",
      "Epoch 17/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.5462\n",
      "Epoch 17: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.5463 - val_loss: 4.1230\n",
      "Epoch 18/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.5127\n",
      "Epoch 18: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.5130 - val_loss: 4.1229\n",
      "Epoch 19/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.4899\n",
      "Epoch 19: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.4899 - val_loss: 4.1235\n",
      "Epoch 20/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.4590\n",
      "Epoch 20: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.4589 - val_loss: 4.1320\n",
      "Epoch 21/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.4349\n",
      "Epoch 21: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.4349 - val_loss: 4.1578\n",
      "Epoch 22/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.4118\n",
      "Epoch 22: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.4118 - val_loss: 4.1536\n",
      "Epoch 23/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.3890\n",
      "Epoch 23: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.3890 - val_loss: 4.1644\n",
      "Epoch 24/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.3675\n",
      "Epoch 24: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.3677 - val_loss: 4.1654\n",
      "Epoch 25/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.3507\n",
      "Epoch 25: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.3508 - val_loss: 4.1709\n",
      "Epoch 26/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.3293\n",
      "Epoch 26: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.3293 - val_loss: 4.1886\n",
      "Epoch 27/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.3113\n",
      "Epoch 27: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.3116 - val_loss: 4.1775\n",
      "Epoch 28/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.2953\n",
      "Epoch 28: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.2955 - val_loss: 4.1943\n",
      "Epoch 29/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.2740\n",
      "Epoch 29: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.2740 - val_loss: 4.2060\n",
      "Epoch 30/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.2655\n",
      "Epoch 30: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.2657 - val_loss: 4.2070\n",
      "Epoch 31/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.2538\n",
      "Epoch 31: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.2537 - val_loss: 4.2110\n",
      "Epoch 32/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.2411\n",
      "Epoch 32: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.2412 - val_loss: 4.2124\n",
      "Epoch 33/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.2255\n",
      "Epoch 33: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.2255 - val_loss: 4.2240\n",
      "Epoch 34/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.2173\n",
      "Epoch 34: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.2173 - val_loss: 4.2278\n",
      "Epoch 35/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.1991\n",
      "Epoch 35: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.1992 - val_loss: 4.2443\n",
      "Epoch 36/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.1889\n",
      "Epoch 36: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.1889 - val_loss: 4.2537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.1726\n",
      "Epoch 37: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.1726 - val_loss: 4.2777\n",
      "Epoch 38/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.1717\n",
      "Epoch 38: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.1717 - val_loss: 4.2573\n",
      "Epoch 39/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.1639\n",
      "Epoch 39: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.1639 - val_loss: 4.2668\n",
      "Epoch 40/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.1576\n",
      "Epoch 40: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.1576 - val_loss: 4.2721\n",
      "Epoch 41/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.1455\n",
      "Epoch 41: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.1455 - val_loss: 4.2958\n",
      "Epoch 42/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.1411\n",
      "Epoch 42: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.1409 - val_loss: 4.2679\n",
      "Epoch 43/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.1321\n",
      "Epoch 43: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.1321 - val_loss: 4.2806\n",
      "Epoch 44/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.1165\n",
      "Epoch 44: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.1168 - val_loss: 4.2753\n",
      "Epoch 45/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.1157\n",
      "Epoch 45: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.1157 - val_loss: 4.2854\n",
      "Epoch 46/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.1005\n",
      "Epoch 46: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.1003 - val_loss: 4.3063\n",
      "Epoch 47/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.0875\n",
      "Epoch 47: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.0876 - val_loss: 4.3008\n",
      "Epoch 48/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.0886\n",
      "Epoch 48: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.0887 - val_loss: 4.3138\n",
      "Epoch 49/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.0883\n",
      "Epoch 49: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.0884 - val_loss: 4.3059\n",
      "Epoch 50/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.0737\n",
      "Epoch 50: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 25s 27ms/step - loss: 3.0738 - val_loss: 4.3164\n",
      "Epoch 51/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.0616\n",
      "Epoch 51: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.0616 - val_loss: 4.3268\n",
      "Epoch 52/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.0617\n",
      "Epoch 52: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.0621 - val_loss: 4.3485\n",
      "Epoch 53/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.0590\n",
      "Epoch 53: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 26ms/step - loss: 3.0590 - val_loss: 4.3431\n",
      "Epoch 54/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.0532\n",
      "Epoch 54: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 24s 26ms/step - loss: 3.0534 - val_loss: 4.3504\n",
      "Epoch 55/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.0508\n",
      "Epoch 55: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.0513 - val_loss: 4.3423\n",
      "Epoch 56/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.0384\n",
      "Epoch 56: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.0384 - val_loss: 4.3458\n",
      "Epoch 57/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.0331\n",
      "Epoch 57: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.0331 - val_loss: 4.3343\n",
      "Epoch 58/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.0296\n",
      "Epoch 58: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.0295 - val_loss: 4.3539\n",
      "Epoch 59/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.0229\n",
      "Epoch 59: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.0229 - val_loss: 4.3836\n",
      "Epoch 60/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 3.0211\n",
      "Epoch 60: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.0218 - val_loss: 4.3808\n",
      "Epoch 61/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.0267\n",
      "Epoch 61: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 23s 25ms/step - loss: 3.0269 - val_loss: 4.3715\n",
      "Epoch 62/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 3.0061\n",
      "Epoch 62: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 3.0061 - val_loss: 4.3816\n",
      "Epoch 63/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 3.0069\n",
      "Epoch 63: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 3.0070 - val_loss: 4.3774\n",
      "Epoch 64/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9971\n",
      "Epoch 64: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 2.9972 - val_loss: 4.3828\n",
      "Epoch 65/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 2.9960\n",
      "Epoch 65: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 2.9962 - val_loss: 4.4089\n",
      "Epoch 66/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 2.9907\n",
      "Epoch 66: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 2.9907 - val_loss: 4.3688\n",
      "Epoch 67/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 2.9811\n",
      "Epoch 67: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 2.9811 - val_loss: 4.3963\n",
      "Epoch 68/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 2.9805\n",
      "Epoch 68: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 25ms/step - loss: 2.9805 - val_loss: 4.3844\n",
      "Epoch 69/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9734\n",
      "Epoch 69: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 22s 24ms/step - loss: 2.9733 - val_loss: 4.3946\n",
      "Epoch 70/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.9711\n",
      "Epoch 70: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9712 - val_loss: 4.4020\n",
      "Epoch 71/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.9697\n",
      "Epoch 71: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9699 - val_loss: 4.4156\n",
      "Epoch 72/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9679\n",
      "Epoch 72: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9679 - val_loss: 4.4047\n",
      "Epoch 73/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.9594\n",
      "Epoch 73: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9596 - val_loss: 4.4015\n",
      "Epoch 74/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9631\n",
      "Epoch 74: val_loss did not improve from 4.10276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9631 - val_loss: 4.3897\n",
      "Epoch 75/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 2.9468\n",
      "Epoch 75: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9466 - val_loss: 4.4480\n",
      "Epoch 76/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9494\n",
      "Epoch 76: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9495 - val_loss: 4.4298\n",
      "Epoch 77/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 2.9481\n",
      "Epoch 77: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 21s 23ms/step - loss: 2.9484 - val_loss: 4.4236\n",
      "Epoch 78/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 2.9464\n",
      "Epoch 78: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 20s 21ms/step - loss: 2.9465 - val_loss: 4.4140\n",
      "Epoch 79/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.9432\n",
      "Epoch 79: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9429 - val_loss: 4.4306\n",
      "Epoch 80/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.9426\n",
      "Epoch 80: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9426 - val_loss: 4.4366\n",
      "Epoch 81/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9367\n",
      "Epoch 81: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9366 - val_loss: 4.4156\n",
      "Epoch 82/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.9378\n",
      "Epoch 82: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9376 - val_loss: 4.4548\n",
      "Epoch 83/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9293\n",
      "Epoch 83: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9290 - val_loss: 4.4400\n",
      "Epoch 84/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 2.9224\n",
      "Epoch 84: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9226 - val_loss: 4.4382\n",
      "Epoch 85/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.9302\n",
      "Epoch 85: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9303 - val_loss: 4.4515\n",
      "Epoch 86/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.9359\n",
      "Epoch 86: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9356 - val_loss: 4.4017\n",
      "Epoch 87/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 2.9100\n",
      "Epoch 87: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9104 - val_loss: 4.4443\n",
      "Epoch 88/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9199\n",
      "Epoch 88: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9201 - val_loss: 4.4761\n",
      "Epoch 89/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.9190\n",
      "Epoch 89: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9194 - val_loss: 4.4608\n",
      "Epoch 90/100\n",
      "913/913 [==============================] - ETA: 0s - loss: 2.9164\n",
      "Epoch 90: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9164 - val_loss: 4.4818\n",
      "Epoch 91/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9104\n",
      "Epoch 91: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9101 - val_loss: 4.4539\n",
      "Epoch 92/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9121\n",
      "Epoch 92: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9122 - val_loss: 4.4911\n",
      "Epoch 93/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 2.9021\n",
      "Epoch 93: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9021 - val_loss: 4.4694\n",
      "Epoch 94/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.9056\n",
      "Epoch 94: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9059 - val_loss: 4.4671\n",
      "Epoch 95/100\n",
      "912/913 [============================>.] - ETA: 0s - loss: 2.9005\n",
      "Epoch 95: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.9007 - val_loss: 4.4785\n",
      "Epoch 96/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.8976\n",
      "Epoch 96: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.8976 - val_loss: 4.4887\n",
      "Epoch 97/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.8926\n",
      "Epoch 97: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.8925 - val_loss: 4.4570\n",
      "Epoch 98/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.8991\n",
      "Epoch 98: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.8993 - val_loss: 4.4725\n",
      "Epoch 99/100\n",
      "910/913 [============================>.] - ETA: 0s - loss: 2.8898\n",
      "Epoch 99: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.8895 - val_loss: 4.4901\n",
      "Epoch 100/100\n",
      "911/913 [============================>.] - ETA: 0s - loss: 2.8912\n",
      "Epoch 100: val_loss did not improve from 4.10276\n",
      "913/913 [==============================] - 19s 21ms/step - loss: 2.8910 - val_loss: 4.4878\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(x_tr), np.array(y_tr), batch_size=64, epochs=100,\n",
    "                   validation_data=(np.array(x_val), np.array(y_val)), verbose=1,\n",
    "                   callbacks=[mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86347bff",
   "metadata": {},
   "source": [
    "Carregando o melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dc0793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4bbb61",
   "metadata": {},
   "source": [
    "Realizando as previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59e893eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 4, 3, 2, 1, 0, 4, 3, 2, 1, 0, 1, 0, 3, 2, 1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 2, 1, 0, 4, 3, 2, 1, 0, 22]\n"
     ]
    }
   ],
   "source": [
    "ind = np.random.randint(0, len(x_val)-1)\n",
    "\n",
    "random_music = x_val[ind]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    random_music = random_music.reshape(1, no_of_timesteps)\n",
    "    \n",
    "    prob = model.predict(random_music)[0]\n",
    "    y_pred = np.argmax(prob, axis=0)\n",
    "    predictions.append(y_pred)\n",
    "    \n",
    "    random_music = np.insert(random_music[0], len(random_music[0]), y_pred)\n",
    "    \n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab92f72",
   "metadata": {},
   "source": [
    "Agora, iremos levar os inteiros de volta as notas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c475e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x))\n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c62200",
   "metadata": {},
   "source": [
    "O passo final é levar de volta as previsões aos arquivos midi, definindo uma função para resolver a tarefa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4014596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output, filename='music.mid'):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    \n",
    "    \n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        #pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            \n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn = int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note    \n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "            \n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        \n",
    "        \n",
    "        offset += 1\n",
    "    \n",
    "    \n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp=filename)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4529f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c93370",
   "metadata": {},
   "source": [
    "Criando uma função que permita a execução do arquivo midi no ambiente do notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9716ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playmidi(filename):\n",
    "    mf = midi.MidiFile()\n",
    "    mf.open(filename)\n",
    "    mf.read()\n",
    "    mf.close()\n",
    "    s = midi.translate.midiFileToStream(mf)\n",
    "    s.show('midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca98289f",
   "metadata": {},
   "source": [
    "Podemos tocar a música ao final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33cc835d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv1272'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv1272');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQACBABNVHJrAAAAFAD/UQMHoSAA/1gEBAIYCIgA/y8ATVRyawAABAMA/wMAAOAAQADAAIgAkEBaAJBEWgCQR1qIAIBAAACARAAAgEcAAJBHWgCQPFqIAIBHAACAPAAAkCdaiACAJwAAkENaAJBGWgCQPFqIAIBDAACARgAAgDwAAJBHWgCQPloAkEFaAJBDWogAgEcAAIA+AACAQQAAgEMAAJBFWogAgEUAAJBEWgCQR1oAkD9aiACARAAAgEcAAIA/AACQRVoAkDxaiACARQAAgDwAAJBDWgCQRFoAkEVaAJBGWogAgEMAAIBEAACARQAAgEYAAJBBWgCQR1qIAIBBAACARwAAkEJaAJBEWogAgEIAAIBEAACQRFoAkEdaAJA/WogAgEQAAIBHAACAPwAAkEVaAJA8WogAgEUAAIA8AACQQ1oAkERaAJBFWgCQRlqIAIBDAACARAAAgEUAAIBGAACQQVoAkEdaiACAQQAAgEcAAJBCWgCQRFqIAIBCAACARAAAkERaAJBHWgCQP1qIAIBEAACARwAAgD8AAJBFWgCQPFqIAIBFAACAPAAAkENaAJBEWgCQRVoAkEZaiACAQwAAgEQAAIBFAACARgAAkEFaAJBHWogAgEEAAIBHAACQQloAkERaiACAQgAAgEQAAJBBWgCQR1qIAIBBAACARwAAkEJaAJBEWogAgEIAAIBEAACQRVoAkDxaiACARQAAgDwAAJBDWgCQRFoAkEVaAJBGWogAgEMAAIBEAACARQAAgEYAAJBBWgCQR1qIAIBBAACARwAAkEJaAJBEWogAgEIAAIBEAACQQVoAkEdaiACAQQAAgEcAAJBCWgCQRFqIAIBCAACARAAAkENaAJBEWgCQRVoAkEZaiACAQwAAgEQAAIBFAACARgAAkEFaAJBHWogAgEEAAIBHAACQQloAkERaiACAQgAAgEQAAJBCWgCQRFqIAIBCAACARAAAkEJaAJBEWogAgEIAAIBEAACQQloAkERaiACAQgAAgEQAAJBCWgCQRFqIAIBCAACARAAAkEJaAJBEWogAgEIAAIBEAACQQloAkERaiACAQgAAgEQAAJBCWgCQRFqIAIBCAACARAAAkERaAJBHWgCQP1qIAIBEAACARwAAgD8AAJBFWgCQPFqIAIBFAACAPAAAkENaAJBEWgCQRVoAkEZaiACAQwAAgEQAAIBFAACARgAAkEFaAJBHWogAgEEAAIBHAACQQloAkERaiACAQgAAgEQAAJBEWgCQR1oAkD9aiACARAAAgEcAAIA/AACQRVoAkDxaiACARQAAgDwAAJBDWgCQRFoAkEVaAJBGWogAgEMAAIBEAACARQAAgEYAAJBBWgCQR1qIAIBBAACARwAAkEJaAJBEWogAgEIAAIBEAACQSlqIAIBKAIgA/y8A');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "playmidi('music.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04471b",
   "metadata": {},
   "source": [
    "### Treinando via LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76ad345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 70, 100)           20400     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 70, 128)           117248    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 204)               26316     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 204)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 313,084\n",
      "Trainable params: 312,572\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model_lstm = Sequential()\n",
    "\n",
    "model_lstm.add(Embedding(len(unique_x), 100, input_length=70, trainable=True))\n",
    "model_lstm.add(LSTM(128, return_sequences=True, recurrent_dropout=0.3))\n",
    "model_lstm.add(LSTM(128))\n",
    "model_lstm.add(BatchNorm())\n",
    "model_lstm.add(Dense(128))\n",
    "model_lstm.add(Activation('relu'))\n",
    "model_lstm.add(BatchNorm())\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(len(unique_y)))\n",
    "model_lstm.add(Activation('softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92bdecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_lstm = ModelCheckpoint('best_model_lstm.h5', monitor='val_loss',\n",
    "                          mode='min', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "897fd5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 4.5136\n",
      "Epoch 1: val_loss improved from inf to 4.45540, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 186s 209ms/step - loss: 4.5136 - val_loss: 4.4554\n",
      "Epoch 2/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 4.2375\n",
      "Epoch 2: val_loss improved from 4.45540 to 4.23938, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 182s 204ms/step - loss: 4.2375 - val_loss: 4.2394\n",
      "Epoch 3/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 4.1592\n",
      "Epoch 3: val_loss did not improve from 4.23938\n",
      "889/889 [==============================] - 182s 204ms/step - loss: 4.1592 - val_loss: 4.2416\n",
      "Epoch 4/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 4.1116\n",
      "Epoch 4: val_loss improved from 4.23938 to 4.18498, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 182s 205ms/step - loss: 4.1116 - val_loss: 4.1850\n",
      "Epoch 5/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 4.0728\n",
      "Epoch 5: val_loss improved from 4.18498 to 4.16502, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 185s 208ms/step - loss: 4.0728 - val_loss: 4.1650\n",
      "Epoch 6/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 4.0392\n",
      "Epoch 6: val_loss improved from 4.16502 to 4.09162, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 182s 205ms/step - loss: 4.0392 - val_loss: 4.0916\n",
      "Epoch 7/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 4.0072\n",
      "Epoch 7: val_loss did not improve from 4.09162\n",
      "889/889 [==============================] - 183s 205ms/step - loss: 4.0072 - val_loss: 4.1023\n",
      "Epoch 8/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.9767\n",
      "Epoch 8: val_loss improved from 4.09162 to 4.04714, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 183s 205ms/step - loss: 3.9767 - val_loss: 4.0471\n",
      "Epoch 9/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.9439\n",
      "Epoch 9: val_loss improved from 4.04714 to 4.01285, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.9439 - val_loss: 4.0129\n",
      "Epoch 10/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.9111\n",
      "Epoch 10: val_loss improved from 4.01285 to 4.01001, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.9111 - val_loss: 4.0100\n",
      "Epoch 11/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.8716\n",
      "Epoch 11: val_loss improved from 4.01001 to 4.00381, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.8716 - val_loss: 4.0038\n",
      "Epoch 12/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.8367\n",
      "Epoch 12: val_loss improved from 4.00381 to 3.98392, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.8367 - val_loss: 3.9839\n",
      "Epoch 13/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.7978\n",
      "Epoch 13: val_loss improved from 3.98392 to 3.97831, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 183s 205ms/step - loss: 3.7978 - val_loss: 3.9783\n",
      "Epoch 14/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.7554\n",
      "Epoch 14: val_loss improved from 3.97831 to 3.97051, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 185s 208ms/step - loss: 3.7554 - val_loss: 3.9705\n",
      "Epoch 15/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.7194\n",
      "Epoch 15: val_loss improved from 3.97051 to 3.96171, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 185s 209ms/step - loss: 3.7194 - val_loss: 3.9617\n",
      "Epoch 16/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.6707\n",
      "Epoch 16: val_loss improved from 3.96171 to 3.95585, saving model to best_model_lstm.h5\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.6707 - val_loss: 3.9559\n",
      "Epoch 17/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.6217\n",
      "Epoch 17: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.6217 - val_loss: 3.9677\n",
      "Epoch 18/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.5842\n",
      "Epoch 18: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 210ms/step - loss: 3.5842 - val_loss: 3.9618\n",
      "Epoch 19/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.5433\n",
      "Epoch 19: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.5433 - val_loss: 3.9657\n",
      "Epoch 20/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.4908\n",
      "Epoch 20: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.4908 - val_loss: 3.9688\n",
      "Epoch 21/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.4447\n",
      "Epoch 21: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.4447 - val_loss: 3.9763\n",
      "Epoch 22/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.4090\n",
      "Epoch 22: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.4090 - val_loss: 3.9706\n",
      "Epoch 23/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.3608\n",
      "Epoch 23: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 3.3608 - val_loss: 3.9876\n",
      "Epoch 24/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.3216\n",
      "Epoch 24: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 206ms/step - loss: 3.3216 - val_loss: 4.0112\n",
      "Epoch 25/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.2836\n",
      "Epoch 25: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 209ms/step - loss: 3.2836 - val_loss: 4.0186\n",
      "Epoch 26/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.2436\n",
      "Epoch 26: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 206ms/step - loss: 3.2436 - val_loss: 4.0188\n",
      "Epoch 27/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.2002\n",
      "Epoch 27: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 3.2002 - val_loss: 4.0355\n",
      "Epoch 28/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.1690\n",
      "Epoch 28: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 3.1690 - val_loss: 4.0524\n",
      "Epoch 29/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.1256\n",
      "Epoch 29: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 3.1256 - val_loss: 4.0752\n",
      "Epoch 30/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.0976\n",
      "Epoch 30: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 3.0976 - val_loss: 4.0897\n",
      "Epoch 31/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.0628\n",
      "Epoch 31: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 3.0628 - val_loss: 4.0902\n",
      "Epoch 32/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 3.0282\n",
      "Epoch 32: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 3.0282 - val_loss: 4.1120\n",
      "Epoch 33/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.9898\n",
      "Epoch 33: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 2.9898 - val_loss: 4.1219\n",
      "Epoch 34/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.9582\n",
      "Epoch 34: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 188s 211ms/step - loss: 2.9582 - val_loss: 4.1478\n",
      "Epoch 35/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.9261\n",
      "Epoch 35: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 205ms/step - loss: 2.9261 - val_loss: 4.1670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.8958\n",
      "Epoch 36: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.8958 - val_loss: 4.1789\n",
      "Epoch 37/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.8698\n",
      "Epoch 37: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.8698 - val_loss: 4.1756\n",
      "Epoch 38/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.8361\n",
      "Epoch 38: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.8361 - val_loss: 4.2102\n",
      "Epoch 39/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.8111\n",
      "Epoch 39: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.8111 - val_loss: 4.2169\n",
      "Epoch 40/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.7804\n",
      "Epoch 40: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.7804 - val_loss: 4.2418\n",
      "Epoch 41/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.7586\n",
      "Epoch 41: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.7586 - val_loss: 4.2407\n",
      "Epoch 42/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.7274\n",
      "Epoch 42: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.7274 - val_loss: 4.2992\n",
      "Epoch 43/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.7082\n",
      "Epoch 43: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.7082 - val_loss: 4.2899\n",
      "Epoch 44/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.6729\n",
      "Epoch 44: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 185s 208ms/step - loss: 2.6729 - val_loss: 4.2975\n",
      "Epoch 45/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.6510\n",
      "Epoch 45: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.6510 - val_loss: 4.3164\n",
      "Epoch 46/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.6350\n",
      "Epoch 46: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.6350 - val_loss: 4.3426\n",
      "Epoch 47/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.5982\n",
      "Epoch 47: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.5982 - val_loss: 4.3799\n",
      "Epoch 48/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.5862\n",
      "Epoch 48: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.5862 - val_loss: 4.3755\n",
      "Epoch 49/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.5578\n",
      "Epoch 49: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.5578 - val_loss: 4.4024\n",
      "Epoch 50/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.5392\n",
      "Epoch 50: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.5392 - val_loss: 4.4123\n",
      "Epoch 51/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.5124\n",
      "Epoch 51: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.5124 - val_loss: 4.4447\n",
      "Epoch 52/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.4952\n",
      "Epoch 52: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.4952 - val_loss: 4.4658\n",
      "Epoch 53/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.4826\n",
      "Epoch 53: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.4826 - val_loss: 4.4606\n",
      "Epoch 54/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.4478\n",
      "Epoch 54: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 188s 211ms/step - loss: 2.4478 - val_loss: 4.5046\n",
      "Epoch 55/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.4331\n",
      "Epoch 55: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.4331 - val_loss: 4.5136\n",
      "Epoch 56/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.4144\n",
      "Epoch 56: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 2.4144 - val_loss: 4.5306\n",
      "Epoch 57/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.3986\n",
      "Epoch 57: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 210ms/step - loss: 2.3986 - val_loss: 4.5355\n",
      "Epoch 58/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.3708\n",
      "Epoch 58: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.3708 - val_loss: 4.5759\n",
      "Epoch 59/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.3577\n",
      "Epoch 59: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.3577 - val_loss: 4.5826\n",
      "Epoch 60/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.3413\n",
      "Epoch 60: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.3413 - val_loss: 4.5645\n",
      "Epoch 61/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.3336\n",
      "Epoch 61: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.3336 - val_loss: 4.5963\n",
      "Epoch 62/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.3180\n",
      "Epoch 62: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.3180 - val_loss: 4.6004\n",
      "Epoch 63/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.2970\n",
      "Epoch 63: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.2970 - val_loss: 4.6490\n",
      "Epoch 64/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.2778\n",
      "Epoch 64: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 185s 208ms/step - loss: 2.2778 - val_loss: 4.6392\n",
      "Epoch 65/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.2597\n",
      "Epoch 65: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.2597 - val_loss: 4.6637\n",
      "Epoch 66/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.2508\n",
      "Epoch 66: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.2508 - val_loss: 4.6749\n",
      "Epoch 67/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.2362\n",
      "Epoch 67: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.2362 - val_loss: 4.6801\n",
      "Epoch 68/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.2199\n",
      "Epoch 68: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.2199 - val_loss: 4.7159\n",
      "Epoch 69/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.2135\n",
      "Epoch 69: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.2135 - val_loss: 4.7158\n",
      "Epoch 70/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.1879\n",
      "Epoch 70: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.1879 - val_loss: 4.7665\n",
      "Epoch 71/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.1735\n",
      "Epoch 71: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.1735 - val_loss: 4.7773\n",
      "Epoch 72/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.1562\n",
      "Epoch 72: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.1562 - val_loss: 4.7850\n",
      "Epoch 73/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.1486\n",
      "Epoch 73: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 209ms/step - loss: 2.1486 - val_loss: 4.8193\n",
      "Epoch 74/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.1357\n",
      "Epoch 74: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 185s 209ms/step - loss: 2.1357 - val_loss: 4.8294\n",
      "Epoch 75/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.1167\n",
      "Epoch 75: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.1167 - val_loss: 4.8286\n",
      "Epoch 76/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.1190\n",
      "Epoch 76: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.1190 - val_loss: 4.8508\n",
      "Epoch 77/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.1027\n",
      "Epoch 77: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.1027 - val_loss: 4.8534\n",
      "Epoch 78/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.0778\n",
      "Epoch 78: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.0778 - val_loss: 4.8691\n",
      "Epoch 79/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.0742\n",
      "Epoch 79: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.0742 - val_loss: 4.8559\n",
      "Epoch 80/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.0609\n",
      "Epoch 80: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.0609 - val_loss: 4.8826\n",
      "Epoch 81/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.0517\n",
      "Epoch 81: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 2.0517 - val_loss: 4.9058\n",
      "Epoch 82/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.0329\n",
      "Epoch 82: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.0329 - val_loss: 4.9372\n",
      "Epoch 83/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.0268\n",
      "Epoch 83: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 209ms/step - loss: 2.0268 - val_loss: 4.9386\n",
      "Epoch 84/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.0143\n",
      "Epoch 84: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.0143 - val_loss: 4.9621\n",
      "Epoch 85/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 2.0084\n",
      "Epoch 85: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 2.0084 - val_loss: 4.9363\n",
      "Epoch 86/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.9945\n",
      "Epoch 86: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.9945 - val_loss: 5.0003\n",
      "Epoch 87/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.9945\n",
      "Epoch 87: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.9945 - val_loss: 4.9895\n",
      "Epoch 88/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.9716\n",
      "Epoch 88: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 206ms/step - loss: 1.9716 - val_loss: 4.9983\n",
      "Epoch 89/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.9711\n",
      "Epoch 89: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.9711 - val_loss: 4.9895\n",
      "Epoch 90/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.9596\n",
      "Epoch 90: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.9596 - val_loss: 5.0301\n",
      "Epoch 91/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.9411\n",
      "Epoch 91: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.9411 - val_loss: 5.0575\n",
      "Epoch 92/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.9326\n",
      "Epoch 92: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 206ms/step - loss: 1.9326 - val_loss: 5.0654\n",
      "Epoch 93/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.9321\n",
      "Epoch 93: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 188s 212ms/step - loss: 1.9321 - val_loss: 5.0900\n",
      "Epoch 94/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.9156\n",
      "Epoch 94: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.9156 - val_loss: 5.0867\n",
      "Epoch 95/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.9078\n",
      "Epoch 95: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.9078 - val_loss: 5.1028\n",
      "Epoch 96/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8969\n",
      "Epoch 96: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.8969 - val_loss: 5.1548\n",
      "Epoch 97/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8939\n",
      "Epoch 97: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 187s 210ms/step - loss: 1.8939 - val_loss: 5.1597\n",
      "Epoch 98/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8859\n",
      "Epoch 98: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.8859 - val_loss: 5.1204\n",
      "Epoch 99/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8746\n",
      "Epoch 99: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.8746 - val_loss: 5.1877\n",
      "Epoch 100/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8666\n",
      "Epoch 100: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.8666 - val_loss: 5.1775\n",
      "Epoch 101/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8515\n",
      "Epoch 101: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.8515 - val_loss: 5.2329\n",
      "Epoch 102/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8603\n",
      "Epoch 102: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.8603 - val_loss: 5.1962\n",
      "Epoch 103/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8444\n",
      "Epoch 103: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 209ms/step - loss: 1.8444 - val_loss: 5.1993\n",
      "Epoch 104/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8250\n",
      "Epoch 104: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.8250 - val_loss: 5.1915\n",
      "Epoch 105/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8179\n",
      "Epoch 105: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.8179 - val_loss: 5.2512\n",
      "Epoch 106/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8215\n",
      "Epoch 106: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.8215 - val_loss: 5.2567\n",
      "Epoch 107/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8067\n",
      "Epoch 107: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.8067 - val_loss: 5.2736\n",
      "Epoch 108/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.8016\n",
      "Epoch 108: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.8016 - val_loss: 5.2541\n",
      "Epoch 109/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7934\n",
      "Epoch 109: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.7934 - val_loss: 5.2823\n",
      "Epoch 110/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889/889 [==============================] - ETA: 0s - loss: 1.7877\n",
      "Epoch 110: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.7877 - val_loss: 5.3048\n",
      "Epoch 111/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7922\n",
      "Epoch 111: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.7922 - val_loss: 5.3164\n",
      "Epoch 112/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7738\n",
      "Epoch 112: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 209ms/step - loss: 1.7738 - val_loss: 5.3212\n",
      "Epoch 113/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7692\n",
      "Epoch 113: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 209ms/step - loss: 1.7692 - val_loss: 5.3394\n",
      "Epoch 114/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7649\n",
      "Epoch 114: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 206ms/step - loss: 1.7649 - val_loss: 5.3522\n",
      "Epoch 115/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7608\n",
      "Epoch 115: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.7608 - val_loss: 5.3644\n",
      "Epoch 116/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7429\n",
      "Epoch 116: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.7429 - val_loss: 5.3627\n",
      "Epoch 117/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7494\n",
      "Epoch 117: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.7494 - val_loss: 5.3598\n",
      "Epoch 118/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7282\n",
      "Epoch 118: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.7282 - val_loss: 5.4200\n",
      "Epoch 119/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7343\n",
      "Epoch 119: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.7343 - val_loss: 5.4259\n",
      "Epoch 120/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7301\n",
      "Epoch 120: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.7301 - val_loss: 5.3726\n",
      "Epoch 121/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7147\n",
      "Epoch 121: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.7147 - val_loss: 5.4336\n",
      "Epoch 122/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7178\n",
      "Epoch 122: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.7178 - val_loss: 5.4031\n",
      "Epoch 123/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7111\n",
      "Epoch 123: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 209ms/step - loss: 1.7111 - val_loss: 5.4202\n",
      "Epoch 124/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7019\n",
      "Epoch 124: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.7019 - val_loss: 5.4388\n",
      "Epoch 125/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.7056\n",
      "Epoch 125: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.7056 - val_loss: 5.4413\n",
      "Epoch 126/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6902\n",
      "Epoch 126: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6902 - val_loss: 5.4638\n",
      "Epoch 127/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6880\n",
      "Epoch 127: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 206ms/step - loss: 1.6880 - val_loss: 5.5139\n",
      "Epoch 128/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6867\n",
      "Epoch 128: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.6867 - val_loss: 5.5011\n",
      "Epoch 129/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6776\n",
      "Epoch 129: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6776 - val_loss: 5.4805\n",
      "Epoch 130/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6743\n",
      "Epoch 130: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.6743 - val_loss: 5.4768\n",
      "Epoch 131/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6645\n",
      "Epoch 131: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.6645 - val_loss: 5.5344\n",
      "Epoch 132/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6646\n",
      "Epoch 132: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 188s 212ms/step - loss: 1.6646 - val_loss: 5.5422\n",
      "Epoch 133/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6586\n",
      "Epoch 133: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6586 - val_loss: 5.5577\n",
      "Epoch 134/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6475\n",
      "Epoch 134: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6475 - val_loss: 5.5616\n",
      "Epoch 135/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6428\n",
      "Epoch 135: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6428 - val_loss: 5.5728\n",
      "Epoch 136/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6507\n",
      "Epoch 136: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 209ms/step - loss: 1.6507 - val_loss: 5.5287\n",
      "Epoch 137/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6472\n",
      "Epoch 137: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6472 - val_loss: 5.5807\n",
      "Epoch 138/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6291\n",
      "Epoch 138: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6291 - val_loss: 5.5671\n",
      "Epoch 139/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6204\n",
      "Epoch 139: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6204 - val_loss: 5.6225\n",
      "Epoch 140/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6235\n",
      "Epoch 140: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 206ms/step - loss: 1.6235 - val_loss: 5.5762\n",
      "Epoch 141/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6201\n",
      "Epoch 141: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6201 - val_loss: 5.5810\n",
      "Epoch 142/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6129\n",
      "Epoch 142: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 186s 209ms/step - loss: 1.6129 - val_loss: 5.6250\n",
      "Epoch 143/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6112\n",
      "Epoch 143: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.6112 - val_loss: 5.6325\n",
      "Epoch 144/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6173\n",
      "Epoch 144: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6173 - val_loss: 5.6068\n",
      "Epoch 145/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6080\n",
      "Epoch 145: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6080 - val_loss: 5.6485\n",
      "Epoch 146/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.6053\n",
      "Epoch 146: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.6053 - val_loss: 5.6091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.5909\n",
      "Epoch 147: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 206ms/step - loss: 1.5909 - val_loss: 5.6443\n",
      "Epoch 148/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.5861\n",
      "Epoch 148: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.5861 - val_loss: 5.6149\n",
      "Epoch 149/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.5784\n",
      "Epoch 149: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 184s 207ms/step - loss: 1.5784 - val_loss: 5.6892\n",
      "Epoch 150/150\n",
      "889/889 [==============================] - ETA: 0s - loss: 1.5701\n",
      "Epoch 150: val_loss did not improve from 3.95585\n",
      "889/889 [==============================] - 183s 206ms/step - loss: 1.5701 - val_loss: 5.7017\n"
     ]
    }
   ],
   "source": [
    "history = model_lstm.fit(np.array(x_tr), np.array(y_tr), batch_size=64, epochs=150,\n",
    "                   validation_data=(np.array(x_val), np.array(y_val)), verbose=1,\n",
    "                   callbacks=[mc_lstm], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acf2a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = load_model('best_model_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea5892a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 181, 12, 181, 12, 181, 12, 181, 12, 181]\n"
     ]
    }
   ],
   "source": [
    "ind = np.random.randint(0, len(x_val)-1)\n",
    "\n",
    "random_music = x_val[ind]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    random_music = random_music.reshape(1, no_of_timesteps)\n",
    "    \n",
    "    prob = model_lstm.predict(random_music)[0]\n",
    "    y_pred = np.argmax(prob, axis=0)\n",
    "    predictions.append(y_pred)\n",
    "    \n",
    "    random_music = np.insert(random_music[0], len(random_music[0]), y_pred)\n",
    "    \n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1052bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x))\n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "191d7b5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_to_midi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_92526/1859429442.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvert_to_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'music_lstm6.mid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_to_midi' is not defined"
     ]
    }
   ],
   "source": [
    "convert_to_midi(predicted_notes, 'music_lstm6.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42922ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv70188'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv70188');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQACBABNVHJrAAAAFAD/UQMHoSAA/1gEBAIYCIgA/y8ATVRyawAAAOMA/wMAAOAAQADAAIgAkD5aAJBAWogAgD4AAIBAAACQPVoAkD9aAJBBWogAgD0AAIA/AACAQQAAkD5aAJBAWogAgD4AAIBAAACQPVoAkD9aAJBBWogAgD0AAIA/AACAQQAAkD5aAJBAWogAgD4AAIBAAACQPVoAkD9aAJBBWogAgD0AAIA/AACAQQAAkD5aAJBAWogAgD4AAIBAAACQPVoAkD9aAJBBWogAgD0AAIA/AACAQQAAkD5aAJBAWogAgD4AAIBAAACQPVoAkD9aAJBBWogAgD0AAIA/AACAQQCIAP8vAA==');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "playmidi('music_lstm6.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
